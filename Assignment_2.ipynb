{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZVgP7Xsy1nzw0HEmstSgf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kylemccullough1/MachineLearning/blob/main/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpycDlfZfamP"
      },
      "outputs": [],
      "source": [
        "# Question 1\n",
        "\n",
        "# The update rule is the gradients derived from the backpropagation through\n",
        "# the MSE loss, the hidden layer, and then the activation of the hidden layer\n",
        "# through the sigmoid activation function. You train this neural network by\n",
        "# going through the forward pass from the inputs into the sigmoid activation\n",
        "# function for the hidden layer, and then through the activation linear for\n",
        "# the output layer, and then through the MSE loss. And then using \n",
        "# backpropagation to go back through the layers over and over until convergence.\n",
        "# This is different from the update rule for binary classification using log loss\n",
        "# because it goes through a different loss function causing a different\n",
        "# backpropagation gradient.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "# Question 2\n",
        "\n",
        "# Dense layer\n",
        "class Layer_Dense:\n",
        "  # Layer initialization\n",
        "  def __init__(self, n_inputs, n_neurons,\n",
        "    weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
        "    bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
        "    # Initialize weights and biases\n",
        "    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros((1, n_neurons))\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Remember input values\n",
        "    self.inputs = inputs\n",
        "    # Calculate output values from inputs, weights and biases\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "  # Backward pass\n",
        "  def backward(self, dvalues):\n",
        "    # Gradients on parameters\n",
        "    self.dweights = np.dot(self.inputs.T, dvalues.T)\n",
        "    self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "    # Gradient on values\n",
        "    self.dinputs = np.dot(dvalues, self.weights)\n",
        "\n",
        "# Sigmoid activation\n",
        "class Activation_Sigmoid:\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Save input and calculate/save output\n",
        "    # of the sigmoid function\n",
        "    self.inputs = inputs\n",
        "    self.output = 1 / (1 + np.exp(-inputs))\n",
        "  # Backward pass\n",
        "  def backward(self, dvalues):\n",
        "    # Derivative - calculates from output of the sigmoid function\n",
        "    print('dvalues', dvalues)\n",
        "    print('output', self.output)\n",
        "    self.dinputs = dvalues * (1 - self.output) * self.output\n",
        "\n",
        "class Activation_ReLU:\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Remember input values\n",
        "    self.inputs = inputs\n",
        "    # Calculate output values from inputs\n",
        "    self.output = np.maximum(0, inputs)\n",
        "  # Backward pass\n",
        "  def backward(self, dvalues):\n",
        "    # Since we need to modify original variable,\n",
        "    # let's make a copy of values first\n",
        "    self.dinputs = dvalues.copy()\n",
        "    # Zero gradient where input values were negative\n",
        "    self.dinputs[self.inputs <= 0] = 0\n",
        "\n",
        "# Linear activation\n",
        "class Activation_Linear:\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Just remember values\n",
        "    self.inputs = inputs\n",
        "    self.output = inputs\n",
        "  # Backward pass\n",
        "  def backward(self, dvalues):\n",
        "    # derivative is 1, 1 * dvalues = dvalues - the chain rule\n",
        "    self.dinputs = dvalues.copy()\n",
        "\n",
        "# Common loss class\n",
        "class Loss:\n",
        "  # Calculates the data and regularization losses\n",
        "  # given model output and ground truth values\n",
        "  def calculate(self, output, y):\n",
        "    # Calculate sample losses\n",
        "    sample_losses = self.forward(output, y)\n",
        "    # Calculate mean loss\n",
        "    data_loss = np.mean(sample_losses)\n",
        "    # Return loss\n",
        "    return data_loss\n",
        "\n",
        "# Mean Squared Error loss\n",
        "class Loss_MeanSquaredError(Loss): \n",
        "  # Forward pass\n",
        "  def forward(self, y_pred, y_true):\n",
        "    # Calculate loss\n",
        "    sample_losses = np.mean((y_true - y_pred.T)**2, axis=-1)\n",
        "    # Return losses\n",
        "    return sample_losses\n",
        "  # Backward pass\n",
        "  def backward(self, dvalues, y_true):\n",
        "    # Number of samples\n",
        "    samples = len(dvalues)\n",
        "    # Number of outputs in every sample\n",
        "    # We'll use the first sample to count them\n",
        "    outputs = len(dvalues[0])\n",
        "    # Gradient on values\n",
        "    self.dinputs = -2 * (y_true - dvalues.T) / outputs\n",
        "    # Normalize gradient\n",
        "    self.dinputs = self.dinputs / samples\n",
        "\n",
        "X_train = np.loadtxt(\"X_train.csv\")\n",
        "# X_train = X_train.T\n",
        "Y_train = np.loadtxt(\"Y_train.csv\") \n",
        "#X_test = np.loadtxt(\"X_test.csv\") \n",
        "# Y_test = np.loadtxt(\"Y_test.csv\") \n",
        "# Create Dense layer with 2 input feature and 100 output values\n",
        "dense1 = Layer_Dense(2, 100)\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_Sigmoid()\n",
        "# Create second Dense layer with 64 input features (as we take output\n",
        "# of previous layer here) and 64 output values\n",
        "dense2 = Layer_Dense(100, 100)\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation2 = Activation_Sigmoid()\n",
        "# Create third Dense layer with 64 input features (as we take output\n",
        "# of previous layer here) and 1 output value\n",
        "dense3 = Layer_Dense(100, 2)\n",
        "# Create Linear activation:\n",
        "# Create loss function\n",
        "activation3 = Activation_Linear()\n",
        "# Create loss function\n",
        "loss_function = Loss_MeanSquaredError()\n",
        "# Accuracy precision for accuracy calculation\n",
        "# There are no really accuracy factor for regression problem,\n",
        "# but we can simulate/approximate it. We'll calculate it by checking\n",
        "# how many values have a difference to their ground truth equivalent\n",
        "# less than given precision\n",
        "# We'll calculate this precision as a fraction of standard deviation\n",
        "# of all the ground truth values\n",
        "accuracy_precision = np.std(Y_train) / 250\n",
        "# Train in loop\n",
        "for epoch in range(10001):\n",
        "  # Perform a forward pass of our training data through this layer\n",
        "  dense1.forward(X_train)\n",
        "  # Perform a forward pass through activation function\n",
        "  # takes the output of first dense layer here\n",
        "  activation1.forward(dense1.output)\n",
        "  # Perform a forward pass through second Dense layer\n",
        "  # takes outputs of activation function\n",
        "  # of first layer as inputs\n",
        "  dense2.forward(activation1.output)\n",
        "  # Perform a forward pass through activation function\n",
        "  # takes the output of second dense layer here\n",
        "  activation2.forward(dense2.output)\n",
        "  # Perform a forward pass through activation function\n",
        "  # takes the output of third dense layer here\n",
        "  activation2.forward(dense2.output)\n",
        "  # Calculate the data loss\n",
        "  dense3.forward(activation2.output)\n",
        "  # Perform a forward pass through activation function\n",
        "  # takes the output of third dense layer here\n",
        "  activation3.forward(dense3.output)\n",
        "  # Calculate the data loss\n",
        "  data_loss = loss_function.calculate(activation2.output, Y_train)\n",
        "  # Calculate overall loss\n",
        "  loss = data_loss\n",
        "  # Calculate accuracy from output of activation2 and targets\n",
        "  # To calculate it we're taking absolute difference between\n",
        "  # predictions and ground truth values and compare if differences\n",
        "  # are lower than given precision value\n",
        "  predictions = activation2.output\n",
        "  accuracy = np.mean(np.absolute(predictions.T - Y_train) < accuracy_precision)\n",
        "  if not epoch % 100:\n",
        "    print(f'epoch: {epoch}, ' +\n",
        "    f'acc: {accuracy:.3f}, ' +\n",
        "    f'loss: {loss:.3f} (' +\n",
        "    f'data_loss: {data_loss:.3f}, ' )\n",
        "  # Backward pass\n",
        "  loss_function.backward(activation3.output, Y_train)\n",
        "  activation3.backward(loss_function.dinputs)\n",
        "  dense3.backward(activation3.dinputs)\n",
        "  activation2.backward(dense3.dinputs)\n",
        "  dense2.backward(activation2.dinputs)\n",
        "  activation1.backward(dense2.dinputs)\n",
        "  dense1.backward(activation1.dinputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "tmbok1Tu5Sqi",
        "outputId": "b5ec8695-5f33-4528-9030-287bcb708e47"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, acc: 0.000, loss: 5709.013 (data_loss: 5709.013, \n",
            "dvalues [[ 0.00685757 -0.02550638]\n",
            " [ 0.00685413 -0.02550956]]\n",
            "output [[0.48507692 0.50230109 0.48095163 ... 0.49126084 0.48845818 0.48431609]\n",
            " [0.48509608 0.50239722 0.48106166 ... 0.4914056  0.48861568 0.48463112]\n",
            " [0.48508615 0.50239475 0.4811726  ... 0.49148588 0.48873255 0.48468525]\n",
            " ...\n",
            " [0.48503332 0.50222321 0.48120139 ... 0.49139381 0.48869082 0.48424628]\n",
            " [0.48506439 0.50229272 0.4810731  ... 0.491345   0.48858376 0.48436036]\n",
            " [0.48510228 0.50232525 0.48073137 ... 0.49111415 0.48823422 0.48425961]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-105-9f741a5c6931>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    172\u001b[0m   \u001b[0mactivation3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdinputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m   \u001b[0mdense3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdinputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m   \u001b[0mactivation2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdinputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m   \u001b[0mdense2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdinputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m   \u001b[0mactivation1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdinputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-105-9f741a5c6931>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dvalues)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dvalues'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdinputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdvalues\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mActivation_ReLU\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,2) (100,100) "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The activation function I chose was just a linear regression activation function that doesn't change the input to output value. This is because the network is for a regression problem, and didn't require any special activation to see the correct results.\n",
        "2. 2 neurons should be in the output layer, because there were 2 input features.\n",
        "# At this point I got stuck on how to fix my program"
      ],
      "metadata": {
        "id": "0hJHatIEQYnj"
      }
    }
  ]
}